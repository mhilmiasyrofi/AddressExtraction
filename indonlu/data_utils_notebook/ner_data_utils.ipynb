{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = {'I-PERSON': 0, 'B-ORGANISATION': 1, 'I-ORGANISATION': 2, 'B-PLACE': 3, 'I-PLACE': 4, 'O': 5, 'B-PERSON': 6}\n",
    "    INDEX2LABEL = {0: 'I-PERSON', 1: 'B-ORGANISATION', 2: 'I-ORGANISATION', 3: 'B-PLACE', 4: 'I-PLACE', 5: 'O', 6: 'B-PERSON'}\n",
    "    NUM_LABELS = 7\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_dataset(path):\n",
    "        # Read file\n",
    "        data = open(path,'r').readlines()\n",
    "\n",
    "        # Prepare buffer\n",
    "        dataset = []\n",
    "        sentence = []\n",
    "        seq_label = []\n",
    "        for line in data:\n",
    "            if len(line.strip()) > 0:\n",
    "                token, label = line[:-1].split('\\t')\n",
    "                sentence.append(token)\n",
    "                seq_label.append(NerDataset.LABEL2INDEX[label])\n",
    "            else:\n",
    "                dataset.append({\n",
    "                    'sentence': sentence,\n",
    "                    'seq_label': seq_label\n",
    "                })\n",
    "                sentence = []\n",
    "                seq_label = []\n",
    "        return dataset\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer):\n",
    "        self.data = NerDataset.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        sentence, seq_label = data['sentence'], data['seq_label']\n",
    "        subwords = []\n",
    "        subword_to_word_indices = []\n",
    "        for word_idx, word in enumerate(sentence):\n",
    "            subword_list = self.tokenizer.encode(word, add_special_tokens=True)\n",
    "            subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "            subwords += subword_list\n",
    "        return np.array(subwords), np.array(subword_to_word_indices), np.array(seq_label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "        \n",
    "class NerDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NerDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_tgt_len = max(map(lambda x: len(x[2]), batch))\n",
    "        \n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        subword_to_word_indices_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        seq_label_batch = np.full((batch_size, max_tgt_len), -100, dtype=np.int64)\n",
    "        \n",
    "        for i, (subwords, subword_to_word_indices, seq_label) in enumerate(batch):\n",
    "            subword_batch[i,:len(subwords)] = subwords\n",
    "            subword_to_word_indices_batch[i,:len(subwords)] = subword_to_word_indices\n",
    "            seq_label_batch[i,:len(seq_label)] = seq_label\n",
    "            \n",
    "        return subword_batch, subword_to_word_indices_batch, seq_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/ner-grit/train_preprocess_0.txt'\n",
    "pretrained_model = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "dataset = NerDataset(dataset_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3347  2226 11687  2050 11937 17157  2722 15488  2050 11265  4590  2072\n",
      "  1018 10930  6292 11905 13320  2022  6820 24206 15125  2050  2273 27875\n",
      "  2072 15488  2226 11265  4590  2072  1018 10930  6292 11905 13320  1012]\n",
      "[ 0  0  1  1  2  2  3  4  4  5  5  5  6  7  7  7  7  8  8  8  9  9 10 10\n",
      " 10 11 11 12 12 12 13 14 14 14 14 15]\n",
      "[5 5 5 5 3 4 4 4 5 5 5 3 4 4 4 5]\n"
     ]
    }
   ],
   "source": [
    "for subwords, subword_to_word_indices, seq_label in dataset:\n",
    "    print(subwords)\n",
    "    print(subword_to_word_indices)\n",
    "    print(seq_label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = NerDataLoader(dataset, batch_size=32, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3347  2226 11687 ...     0     0     0]\n",
      " [ 2022 13639 10286 ...     0     0     0]\n",
      " [12183 22134 19817 ...     0     0     0]\n",
      " ...\n",
      " [11050  2078  7367 ...     0     0     0]\n",
      " [24595  1038 15006 ...     0     0     0]\n",
      " [ 7367 16078  4886 ...     0     0     0]] [[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [[   5    5    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " ...\n",
      " [   6    5    5 ... -100 -100 -100]\n",
      " [   6    0    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]]\n",
      "[[ 2022 22381  2863 ...     0     0     0]\n",
      " [ 7370  2041  2905 ...     0     0     0]\n",
      " [ 7279  4305  3089 ...     0     0     0]\n",
      " ...\n",
      " [12183 22134 14255 ...     0     0     0]\n",
      " [28144  5162 25060 ...     0     0     0]\n",
      " [ 2474 12193  2566 ...     0     0     0]] [[0 0 0 ... 0 0 0]\n",
      " [0 1 2 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]] [[   5    5    6 ... -100 -100 -100]\n",
      " [   1    2    2 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " ...\n",
      " [   5    5    1 ... -100 -100 -100]\n",
      " [   6    5    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]]\n",
      "[[ 7367 15987  2273 ...     0     0     0]\n",
      " [24547  4887 14289 ...     0     0     0]\n",
      " [27356 11795  3148 ...     0     0     0]\n",
      " ...\n",
      " [17710  7630 10404 ...     0     0     0]\n",
      " [ 9587  7507 14760 ...     0     0     0]\n",
      " [ 4487 17710  7630 ...  6977 17175  1012]] [[ 0  0  1 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  1  1 ... 45 45 46]] [[   5    5    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " ...\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " [   6    0    5 ... -100 -100 -100]\n",
      " [   5    3    4 ... -100 -100 -100]]\n",
      "[[ 3769  7068  5332 ...     0     0     0]\n",
      " [ 3282  5575  2033 ...     0     0     0]\n",
      " [14855  7630  2099 ...     0     0     0]\n",
      " ...\n",
      " [ 4487  3520  4691 ...     0     0     0]\n",
      " [10514 22272  1006 ...     0     0     0]\n",
      " [17710 12274 11692 ...     0     0     0]] [[0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [[   5    5    5 ... -100 -100 -100]\n",
      " [   3    4    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " ...\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " [   5    5    5 ...    5    5    5]\n",
      " [   5    5    5 ... -100 -100 -100]]\n",
      "[[22939  2566 15272 ...     0     0     0]\n",
      " [24264  2273  2850 ...     0     0     0]\n",
      " [11687  2050 10514 ...     0     0     0]\n",
      " ...\n",
      " [10654  8197  2099 ...     0     0     0]\n",
      " [17710  3775  2912 ...     0     0     0]\n",
      " [ 2613  1051 13469 ...     0     0     0]] [[0 1 1 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 1 ... 0 0 0]] [[   5    5    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " ...\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " [   5    5    3 ... -100 -100 -100]\n",
      " [   1    2    5 ... -100 -100 -100]]\n",
      "[[ 4487 10609  8525 ...     0     0     0]\n",
      " [ 2566 24238  2319 ...     0     0     0]\n",
      " [17712 18410  2102 ...     0     0     0]\n",
      " ...\n",
      " [ 8840 22879  2080 ...     0     0     0]\n",
      " [ 8670 24498 16595 ...     0     0     0]\n",
      " [ 2275 10581  2232 ...     0     0     0]] [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [[   5    5    5 ... -100 -100 -100]\n",
      " [   5    5    6 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " ...\n",
      " [   6    5    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]\n",
      " [   5    5    5 ... -100 -100 -100]]\n",
      "CPU times: user 71.7 ms, sys: 268 ms, total: 340 ms\n",
      "Wall time: 651 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, (subwords, subword_to_word_indices, seq_label) in enumerate(loader):\n",
    "    print(subwords, subword_to_word_indices, seq_label)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
