{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AspectExtractionDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = {'I-SENTIMENT': 0, 'O': 1, 'I-ASPECT': 2, 'B-SENTIMENT': 3, 'B-ASPECT': 4}\n",
    "    INDEX2LABEL = {0: 'I-SENTIMENT', 1: 'O', 2: 'I-ASPECT', 3: 'B-SENTIMENT', 4: 'B-ASPECT'}\n",
    "    NUM_LABELS = 5\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_dataset(path):\n",
    "        # Read file\n",
    "        data = open(path,'r').readlines()\n",
    "\n",
    "        # Prepare buffer\n",
    "        dataset = []\n",
    "        sentence = []\n",
    "        seq_label = []\n",
    "        for line in data:\n",
    "            if '\\t' in line:\n",
    "                token, label = line[:-1].split('\\t')\n",
    "                sentence.append(token)\n",
    "                seq_label.append(AspectExtractionDataset.LABEL2INDEX[label])\n",
    "            else:\n",
    "                dataset.append({\n",
    "                    'sentence': sentence,\n",
    "                    'seq_label': seq_label\n",
    "                })\n",
    "                sentence = []\n",
    "                seq_label = []\n",
    "        return dataset\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer):\n",
    "        self.data = AspectExtractionDataset.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        sentence, seq_label = data['sentence'], data['seq_label']\n",
    "        subwords = []\n",
    "        subword_to_word_indices = []\n",
    "        for word_idx, word in enumerate(sentence):\n",
    "            subword_list = self.tokenizer.encode(word, add_special_tokens=True)\n",
    "            subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "            subwords += subword_list\n",
    "        return np.array(subwords), np.array(subword_to_word_indices), np.array(seq_label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "        \n",
    "class AspectExtractionDataLoader(DataLoader):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AspectExtractionDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_tgt_len = max(map(lambda x: len(x[2]), batch))\n",
    "        \n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        subword_to_word_indices_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        seq_label_batch = np.full((batch_size, max_tgt_len), -100, dtype=np.int64)\n",
    "        \n",
    "        for i, (subwords, subword_to_word_indices, seq_label) in enumerate(batch):\n",
    "            subword_batch[i,:len(subwords)] = subwords\n",
    "            subword_to_word_indices_batch[i,:len(subwords)] = subword_to_word_indices\n",
    "            seq_label_batch[i,:len(seq_label)] = seq_label\n",
    "            \n",
    "        return subword_batch, subword_to_word_indices_batch, seq_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../data/aspect-extraction-review-airy/train.txt'\n",
    "pretrained_model = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "dataset = AspectExtractionDataset(dataset_path, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27829  2906  2360  2050 15262  6358  9305  2050  4487  9353 14841 23597\n",
      "  2022 12881  5575  5332 15502  1012  4907 26536  2050 15536  8873 12849\n",
      "  2638  5705  2072 13970 24388 17079  4014  1012]\n",
      "[ 0  0  1  1  2  3  3  3  4  5  6  6  7  7  7  7  8  9 10 11 11 12 12 13\n",
      " 13 13 13 14 14 15 15 16]\n",
      "[1 1 1 1 1 4 3 0 0 1 1 1 4 2 3 0 1]\n"
     ]
    }
   ],
   "source": [
    "for subwords, subword_to_word_indices, seq_label in dataset:\n",
    "    x = (subwords, subword_to_word_indices, seq_label)\n",
    "    print(subwords)\n",
    "    print(subword_to_word_indices)\n",
    "    print(seq_label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = AspectExtractionDataLoader(dataset, batch_size=1024, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27829  2906  2360 ...     0     0     0]\n",
      " [ 8915  8737  4017 ...     0     0     0]\n",
      " [ 7929  2063  9748 ...     0     0     0]\n",
      " ...\n",
      " [26209  2226  8945 ...     0     0     0]\n",
      " [27829  2906  2022 ...     0     0     0]\n",
      " [20377  2050 16137 ...     0     0     0]] [[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]] [[   1    1    1 ... -100 -100 -100]\n",
      " [   4    3    1 ... -100 -100 -100]\n",
      " [   3    0    1 ... -100 -100 -100]\n",
      " ...\n",
      " [   3    1    1 ... -100 -100 -100]\n",
      " [   4    3    1 ... -100 -100 -100]\n",
      " [   1    1    1 ... -100 -100 -100]]\n",
      "[[19379  4430 26927 ...     0     0     0]\n",
      " [ 4487 11493  2072 ...     0     0     0]\n",
      " [18178  5480  1999 ...     0     0     0]\n",
      " ...\n",
      " [14841 23597  7367 ...     0     0     0]\n",
      " [ 8915  8737  4017 ...     0     0     0]\n",
      " [ 7658  3148 13970 ...     0     0     0]] [[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]] [[   1    1    1 ... -100 -100 -100]\n",
      " [   1    4    3 ... -100 -100 -100]\n",
      " [   4    2    3 ... -100 -100 -100]\n",
      " ...\n",
      " [   3    0    4 ... -100 -100 -100]\n",
      " [   4    3    0 ... -100 -100 -100]\n",
      " [   1    1    1 ... -100 -100 -100]]\n",
      "[[16405  3022  9748 ...     0     0     0]\n",
      " [ 6904 27572 24317 ...     0     0     0]\n",
      " [21877 22923  7229 ...     0     0     0]\n",
      " ...\n",
      " [27829  2906 12849 ...  2009  2226  1012]\n",
      " [14841 23597 15262 ...     0     0     0]\n",
      " [21790  2243  1012 ...     0     0     0]] [[  0   0   1 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   1 ... 112 112 113]\n",
      " [  0   0   1 ...   0   0   0]\n",
      " [  0   0   1 ...   0   0   0]] [[   3    0    1 ... -100 -100 -100]\n",
      " [   1    1    1 ... -100 -100 -100]\n",
      " [   4    1    1 ... -100 -100 -100]\n",
      " ...\n",
      " [   4    3    1 ...    1    1    1]\n",
      " [   3    0    4 ... -100 -100 -100]\n",
      " [   3    1    4 ... -100 -100 -100]]\n",
      "[[ 8934  2050  7658 ...     0     0     0]\n",
      " [27829  2906  2022 ...     0     0     0]\n",
      " [ 2022  2869 19190 ...     0     0     0]\n",
      " ...\n",
      " [ 3309 11320 27871 ...     0     0     0]\n",
      " [14841 23597 15262 ...     0     0     0]\n",
      " [21877 22923  7229 ...     0     0     0]] [[0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 1 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]] [[   1    1    1 ... -100 -100 -100]\n",
      " [   4    3    1 ... -100 -100 -100]\n",
      " [   3    1    3 ... -100 -100 -100]\n",
      " ...\n",
      " [   4    3    0 ... -100 -100 -100]\n",
      " [   1    1    1 ... -100 -100 -100]\n",
      " [   4    1    3 ... -100 -100 -100]]\n",
      "CPU times: user 79.6 ms, sys: 409 ms, total: 488 ms\n",
      "Wall time: 2.31 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i, (subwords, subword_to_word_indices, seq_label) in enumerate(loader):\n",
    "    print(subwords, subword_to_word_indices, seq_label)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
